# %% [markdown]
# # Comprehensive Data Science Project Pipeline
# ---

# %%
# 1Ô∏è‚É£ Data Loading & Understanding
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('student_data.csv')

print("--- Dataset Shape ---")
print(df.shape)

print("\n--- First 5 Rows ---")
display(df.head())

print("\n--- Data Types ---")
print(df.dtypes)

# Identify numerical and categorical features
numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = df.select_dtypes(exclude=[np.number]).columns.tolist()

print(f"\nNumerical Features ({len(numerical_features)}): {numerical_features}")
print(f"Categorical Features ({len(categorical_features)}): {categorical_features}")

# %%
# 2Ô∏è‚É£ Data Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Remove duplicated records
initial_len = len(df)
df = df.drop_duplicates()
print(f"Removed {initial_len - len(df)} duplicate records.")

# Handle inconsistent or incorrect data (Example: stripping whitespace from categorical data)
for col in categorical_features:
    if df[col].dtype == 'object':
        df[col] = df[col].astype(str).str.strip()

# Encode categorical features (Label Encoding for binary/ordinal, One-Hot can be applied here too)
le = LabelEncoder()
df_encoded = df.copy()
for col in categorical_features:
    df_encoded[col] = le.fit_transform(df[col])

# Normalize or standardize numerical features
scaler = StandardScaler()
df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])

print("Categorical features encoded and numerical features standardized.")
display(df_encoded.head())

# %%
# 3Ô∏è‚É£ Missing Values Treatment

print("--- Missing Values Before Treatment ---")
print(df.isnull().sum())

# The student dataset is typically clean, but we implement handling for completeness
# Mean (for numerical features): Suitable for normal distributions
# Median (if data is skewed): Robust to outliers
# Mode (for categorical features): Best for non-numeric classes

for col in numerical_features:
    if df[col].isnull().any():
        # Check for skewness to decide between mean/median
        if abs(df[col].skew()) > 1:
            df[col].fillna(df[col].median(), inplace=True)
            print(f"Filled {col} with MEDIAN (skewed data).")
        else:
            df[col].fillna(df[col].mean(), inplace=True)
            print(f"Filled {col} with MEAN (normal data).")

for col in categorical_features:
    if df[col].isnull().any():
        df[col].fillna(df[col].mode()[0], inplace=True)
        print(f"Filled {col} with MODE (categorical).")

print("\nMissing values search complete.")

# %%
# 4Ô∏è‚É£ Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid", palette="muted")
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Histogram
sns.histplot(df['G3'], kde=True, ax=axes[0, 0], color='teal')
axes[0, 0].set_title('Distribution of Final Grades (G3)')

# Boxplot for outliers
sns.boxplot(x='absences', data=df, ax=axes[0, 1], color='coral')
axes[0, 1].set_title('Detecting Outliers in Absences')

# Scatter plot
sns.scatterplot(x='G1', y='G3', hue='sex', data=df, ax=axes[1, 0])
axes[1, 0].set_title('Correlation between G1 and G3 (Final Grade)')

# Bar chart for categorical
sns.countplot(x='health', data=df, ax=axes[1, 1], palette='viridis')
axes[1, 1].set_title('Student Health Status Count')

plt.tight_layout()
plt.show()

# %%
# 5Ô∏è‚É£ Binning Process

# Applying binning on 'age' or 'G3' (numerical)
# Use equal-frequency binning (Quantile-based)
df['Grade_Bin'] = pd.qcut(df['G3'], q=3, labels=['Low', 'Medium', 'High'])

print("Binned 'G3' into 3 quantiles (Low, Medium, High):")
print(df['Grade_Bin'].value_counts())

# Visualize binned feature
plt.figure(figsize=(8, 5))
sns.countplot(x='Grade_Bin', data=df, palette='magma')
plt.title('Visualization of Binned Final Grades')
plt.show()

# %%
# 6Ô∏è‚É£ Descriptive Data Analysis
stats = df[numerical_features].agg(['min', 'max', 'mean', 'var', 'std', 'skew', 'kurtosis']).T
print("--- Descriptive Statistics ---")
display(stats)

print("\n--- Explanation ---")
print("Mean: Average value, center of gravity.")
print("Variance/Std Dev: Measure of data spread.")
print("Skewness: Measures asymmetry. Positive = right tail, Negative = left tail.")
print("Kurtosis: Measures 'tailedness'. High = more outliers.")

# %%
# 7Ô∏è‚É£ Statistical Data Analysis
from scipy import stats as scipy_stats

print("--- Covariance Matrix ---")
display(df[numerical_features].cov())

print("\n--- Correlation Matrix ---")
corr = df[numerical_features].corr()
display(corr)

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Chi-square test (sex vs Internet Access)
contingency = pd.crosstab(df['sex'], df['internet'])
chi2, p, dof, ex = scipy_stats.chi2_contingency(contingency)
print(f"\nChi-square test (Sex vs Internet): p-value = {p:.4f}")

# T-test (Compare G3 means between Sex F and M)
f_grades = df[df['sex'] == 'F']['G3']
m_grades = df[df['sex'] == 'M']['G3']
t_stat, p_val = scipy_stats.ttest_ind(f_grades, m_grades)
print(f"T-test (G3 between Sexes): p-value = {p_val:.4f}")

# ANOVA (Compare G3 across traveltime groups)
groups = [df[df['traveltime'] == t]['G3'] for t in df['traveltime'].unique()]
f_stat, p_val_anova = scipy_stats.f_oneway(*groups)
print(f"ANOVA (G3 across Traveltime groups): p-value = {p_val_anova:.4f}")

# %%
# 8Ô∏è‚É£ Good Charts for Data Interpretation

plt.figure(figsize=(10, 6))
sns.violinplot(x='studytime', y='G3', data=df, inner='quartile', palette='Set2')
plt.title('Impact of Study Time on Final Grade (Violin Plot)')
plt.xlabel('Study Time (1: <2h, 4: >10h)')
plt.ylabel('Final Grade (G3)')
plt.show()

print("Interpretation: The violin plot shows the distribution density of grades. High study time (4) tends to shrink the lower tail, suggesting better performance stability.")

# %%
# 9Ô∏è‚É£ Feature Reduction and Selection
from sklearn.feature_selection import SelectKBest, f_classif

X = df_encoded.drop('G3', axis=1, errors='ignore')
y = df['G3'] # Target can be continuous or class for selection

# Identify redundant features using correlation
corr_matrix = X.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]
print(f"Redundant features to drop based on high correlation (>0.9): {to_drop}")

# Statistical feature selection
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X, (y > 10).astype(int)) # selection for binary outcome
selected_indices = selector.get_support(indices=True)
selected_features = X.columns[selected_indices].tolist()

print(f"\nTop 10 selected features: {selected_features}")
print("Impact: Reduction improves model speed and prevents overfitting on noise.")

# %%
# üîü Dimensionality Reduction Techniques
from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Use standardized data
# Using errors='ignore' because Grade_Bin might only exist in df, not df_encoded copy
X_scaled = df_encoded.drop(['G3', 'Grade_Bin'], axis=1, errors='ignore')
y_class = (df['G3'] > 10).astype(int)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
print(f"PCA Explained Variance: {pca.explained_variance_ratio_}")

# LDA
lda = LinearDiscriminantAnalysis(n_components=1)
X_lda = lda.fit_transform(X_scaled, y_class)

# Kernel PCA (Non-linear)
kpca = KernelPCA(n_components=2, kernel='rbf')
X_kpca = kpca.fit_transform(X_scaled[:100]) # Sample for speed

# SVD
svd = TruncatedSVD(n_components=2)
X_svd = svd.fit_transform(X_scaled)

print("Dimensionality reduction techniques applied successfully.")

# %%
# 1Ô∏è‚É£1Ô∏è‚É£ Dataset Splitting
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_class, test_size=0.20, random_state=42)

print(f"Training set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")
print("Importance: Splitting allows us to evaluate the model on unseen data, preventing memorization/overfitting.")

# %%
# 1Ô∏è‚É£2Ô∏è‚É£ Model Implementations
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA_Model

models = {
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(criterion='entropy'),
    "LDA": LDA_Model(),
    "Neural Network": MLPClassifier(max_iter=500, hidden_layer_sizes=(10, 5)),
    "KNN": KNeighborsClassifier(n_neighbors=5, metric='minkowski'),
    "Logistic Regression": LogisticRegression(max_iter=500)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    results[name] = model.predict(X_test)

# Linear Regression (Continuous Target)
lin_reg = LinearRegression()
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, df['G3'], test_size=0.2, random_state=42)
lin_reg.fit(X_train_reg, y_train_reg)
reg_pred = lin_reg.predict(X_test_reg)

print("Model training and testing complete.")

# %%
# 1Ô∏è‚É£3Ô∏è‚É£ Model Evaluation
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, mean_absolute_error, mean_squared_error, r2_score

print("--- Classification Metrics ---")
class_metrics = {}
for name, preds in results.items():
    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds)
    class_metrics[name] = acc
    print(f"{name:20}: Accuracy={acc:.4f}, F1-Score={f1:.4f}")

print("\n--- Regression Metrics ---")
mae = mean_absolute_error(y_test_reg, reg_pred)
rmse = np.sqrt(mean_squared_error(y_test_reg, reg_pred))
r2 = r2_score(y_test_reg, reg_pred)
print(f"Linear Regression: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}")

# ROC Curve for Logistic Regression
probs = models['Logistic Regression'].predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, probs)
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc(fpr, tpr):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve (Logistic Regression)')
plt.legend()
plt.show()

# %%
# 1Ô∏è‚É£4Ô∏è‚É£ K-Fold Cross Validation
from sklearn.model_selection import cross_val_score

cv_results = {}
for name, model in models.items():
    scores = cross_val_score(model, X_scaled, y_class, cv=5)
    cv_results[name] = scores.mean()
    print(f"{name:20}: CV Avg Accuracy = {scores.mean():.4f}")

print("\nImportance: Cross-validation ensures the model's reliability by testing on multiple different folds of the data.")

# %%
# 1Ô∏è‚É£5Ô∏è‚É£ Statistical Interpretation of Results

# Example: Compare scores of two models using a T-test
scores_dt = cross_val_score(models['Decision Tree'], X_scaled, y_class, cv=10)
scores_lr = cross_val_score(models['Logistic Regression'], X_scaled, y_class, cv=10)

t_stat, p_val = scipy_stats.ttest_rel(scores_dt, scores_lr)
print(f"Paired T-test between Decision Tree and Logistic Regression: p-value = {p_val:.4f}")
if p_val < 0.05: 
    print("Conclusion: There is a statistically significant difference in performance.")
else:
    print("Conclusion: No statistically significant difference in performance.")

# %%
# 1Ô∏è‚É£6Ô∏è‚É£ Overfitting and Underfitting Analysis
from sklearn.model_selection import learning_curve

def plot_learning_curve(model, name):
    train_sizes, train_scores, test_scores = learning_curve(model, X_scaled, y_class, cv=5)
    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label="Training score")
    plt.plot(train_sizes, np.mean(test_scores, axis=1), 's-', label="Cross-validation score")
    plt.title(f"Learning Curve: {name}")
    plt.xlabel("Training size")
    plt.ylabel("Score")
    plt.legend(loc="best")
    plt.show()

plot_learning_curve(models['Decision Tree'], "Decision Tree")
print("Interpretation: If Training >> Validation, the model is OVERFITTING. If both are low, it is UNDERFITTING.")

# %%
# 1Ô∏è‚É£7Ô∏è‚É£ Results Visualization

plt.figure(figsize=(12, 6))
names = list(cv_results.keys())
values = list(cv_results.values())
sns.barplot(x=names, y=values, palette='coolwarm')
plt.ylim(0, 1)
plt.title('Comparison of Model Performance (K-Fold Avg Accuracy)')
plt.ylabel('Accuracy')
for i, v in enumerate(values):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center')
plt.show()

print("Final Summary: The bar chart clearly indicates which model generalized best across the student dataset.")

# %%



